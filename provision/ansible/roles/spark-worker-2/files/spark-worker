#!/bin/bash
#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Starts a Spark Worker ( ie Slave )
#
# chkconfig: 345 86 14
# description: Spark Worker
#
### BEGIN INIT INFO
# Provides:          spark-worker
# Short-Description: Spark Worker
# Default-Start:     3 4 5
# Default-Stop:      0 1 2 6
# Required-Start:    $syslog $remote_fs
# Required-Stop:     $syslog $remote_fs
# Should-Start:
# Should-Stop:
### END INIT INFO

# Script location using the current version of spark installed
SBIN_DIR="/usr/lib/spark/sbin"

# Spark Master URL
SPARK_MASTER_URL="spark://spark-master:7077"

# The User, at the moment we will use root. The spark-daemon.sh uses this
USER="root"

# The server hostname, This will be used to change the hostname in the Spark UI
# NOTE: This does not appear to be necessary but nice to have.  Makes the UI easier to tell which server is which
WORKER_HOSTNAME=`hostname`

# Spark worker has a work dir and a tmp dir.  Set the tmp dir to also point to the mounted RAID-0
SPARK_LOCAL_DIRS="/usr/lib/spark/tmp"

start() {

  rm -rf /var/log/spark/*.*

  exec "${SBIN_DIR}"/start-slave.sh "${SPARK_MASTER_URL}" --host "${WORKER_HOSTNAME}"

}

stop() {

  exec "${SBIN_DIR}"/stop-slave.sh

}

restart() {
  stop
  start
}

checkstatus() {

  exec ${SBIN_DIR}/spark-daemon.sh status org.apache.spark.deploy.worker.Worker 1

}

check_for_root() {
  if [ $(id -ur) -ne 0 ]; then
    echo 'Error: root user required'
    echo
    exit 1
  fi
}

service() {
  case "$1" in
    start)
      check_for_root
      start
      ;;
    stop)
      check_for_root
      stop
      ;;
    status)
      checkstatus
      RETVAL=$?
      ;;
    restart)
      check_for_root
      restart
      ;;
    *)
      echo $"Usage: $0 {start|stop|status|restart}"
      exit 1
  esac
}

service "$@"
